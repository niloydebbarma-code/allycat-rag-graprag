{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG on HTML documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_config import MY_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2: Setup Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If connection to https://huggingface.co/ failed, uncomment the following path\n",
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/my-stuff/projects/ai-alliance/allycat-1/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name = MY_CONFIG.EMBEDDING_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Connect to Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sujee/my-stuff/projects/ai-alliance/allycat-1/.venv/lib/python3.11/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "2025-07-14 00:23:38,214 [DEBUG][_create_connection]: Created new connection using: async-workspace/rag_website_milvus.db (async_milvus_client.py:599)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Milvus instance:  workspace/rag_website_milvus.db\n"
     ]
    }
   ],
   "source": [
    "# connect to vector db\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri = MY_CONFIG.DB_URI ,\n",
    "    dim = MY_CONFIG.EMBEDDING_LENGTH , \n",
    "    collection_name = MY_CONFIG.COLLECTION_NAME,\n",
    "    overwrite=False  # so we load the index from db\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "print (\"✅ Connected to Milvus instance: \", MY_CONFIG.DB_URI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4: Load Document Index from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded index from vector db: workspace/rag_website_milvus.db\n",
      "CPU times: user 109 ms, sys: 16.8 ms, total: 126 ms\n",
      "Wall time: 123 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, storage_context=storage_context)\n",
    "\n",
    "print (\"✅ Loaded index from vector db:\", MY_CONFIG.DB_URI )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-5: Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using LLM model : ollama/gemma3:1b\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.litellm import LiteLLM\n",
    "\n",
    "# Setup LLM\n",
    "print (f\"✅ Using LLM model : {MY_CONFIG.LLM_MODEL}\")\n",
    "Settings.llm = LiteLLM (\n",
    "        model=MY_CONFIG.LLM_MODEL,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-6: Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AI Alliance is an international community of researchers, developers, and organizational leaders committed to fostering open innovation across the AI technology landscape to accelerate progress, improve safety, security, diversity and economic competitiveness in AI.\n"
     ]
    }
   ],
   "source": [
    "import query_utils\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('What is AI Alliance?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AI Alliance is focused on fostering an open community and enabling developers and researchers to accelerate responsible innovation in AI while ensuring scientific rigor, trust, safety, security, diversity and economic competitiveness.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('What are the main focus areas of AI Alliance?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, here are some of the AI Alliance projects mentioned:\n",
      "\n",
      "*   FPT Software\n",
      "*   Hebrew University of Jerusalem\n",
      "*   Hugging Face\n",
      "*   IBM\n",
      "*   Abdus Salam International Centre for Theoretical Physics (ICTP)\n",
      "*   Imperial College London\n",
      "*   Indian Institute of Technology Bombay\n",
      "*   Institute for Computer Science, Artificial Intelligence\n",
      "*   Intel\n",
      "*   Keio University\n",
      "*   LangChain\n",
      "*   LlamaIndex\n",
      "*   Linux Foundation\n",
      "*   Mass Open Cloud Alliance, operated by Boston University and Harvard\n",
      "*   Meta\n",
      "*   Mohamed bin Zayed University of Artificial Intelligence\n",
      "*   MLCommons\n",
      "*   National Aeronautics and Space Administration\n",
      "*   National Science Foundation\n",
      "*   New York University\n",
      "*   NumFOCUS\n",
      "*   OpenTeams\n",
      "*   Oracle\n",
      "*   Partnership on AI\n",
      "*   Quansight\n",
      "*   Red Hat\n",
      "*   Rensselaer Polytechnic Institute\n",
      "*   Roadzen\n",
      "*   Sakana AI\n",
      "*   SB Intuitions\n",
      "*   ServiceNow\n",
      "*   Silo AI\n",
      "*   Simons Foundation\n",
      "*   Sony Group\n",
      "*   Stability AI\n",
      "*   Together AI\n",
      "*   TU Munich\n",
      "*   UC Berkeley College of Computing, Data Science, and Society\n",
      "*   University of Illinois Urbana-Champaign\n",
      "*   The University of Notre Dame\n",
      "*   The University of Texas at Austin\n",
      "*   The University of Tokyo\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('What are some ai alliance projects?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On August 8th, The AI Alliance hosted Open Source AI Demo Night in San Francisco.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('Where was the demo night held?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AI Alliance is focused on developing and sharing foundational models for science.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('What is the AI Alliance doing in the area of material science?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By submitting this form, you agree that the AI Alliance will collect and process the personal information you provide to keep you informed about AI Alliance initiatives and enable your involvement in AI Alliance activities. Additionally, you agree that the AI Alliance may share the personal information you provide with its member organizations so that they may communicate with you about AI Alliance initiatives and your involvement in AI Alliance activities.\n",
      "\n",
      "You may withdraw your consent for the processing of your personal information by the AI Alliance. Please contact us to request a permanent deletion.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('How do I join the AI Alliance?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context does not provide information about the moon landing.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "query = query_utils.tweak_query('When was the moon landing?', MY_CONFIG.LLM_MODEL)\n",
    "res = query_engine.query(query)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allycat-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
