
## All of the settings below are optional
## Uncomment to specify a value

## ----------- workspace to save downloaded files / artifacts -----------
# WORKSPACE_DIR = 'workspace'

## --------  Crawl settings  -----------
## how long to wait between requests (in seconds)
# WAITTIME_BETWEEN_REQUESTS = 0.1

## ----------- Embedding models -----------
## Find embedding models: https://huggingface.co/spaces/mteb/leaderboard

## size : 61 M
# EMBEDDING_MODEL = 'ibm-granite/granite-embedding-30m-english'
# EMBEDDING_LENGTH = 384

## size: 129 M
# EMBEDDING_MODEL = 'BAAI/bge-small-en-v1.5'
# EMBEDDING_LENGTH = 384

## Size : 219 M
# EMBEDDING_MODEL = 'ibm-granite/granite-embedding-107m-multilingual'
# EMBEDDING_LENGTH = 384

## size : 471 M
# EMBEDDING_MODEL = 'intfloat/multilingual-e5-small'
# EMBEDDING_LENGTH = 384

## ----------- Chunking  -----------
# CHUNK_SIZE = 512
# CHUNK_OVERLAP = 20

## ------- LLMs for RAG (Chat) --------

# LLM Runtime Environment
# Set this to select which LLM provider to use for query synthesis
# Options: local_ollama, nebius, replicate, cerebras
# Example:
# LLM_RUN_ENV = 'local_ollama'   # For local Ollama
# LLM_RUN_ENV = 'nebius'         # For Nebius cloud
# LLM_RUN_ENV = 'replicate'      # For Replicate cloud
# LLM_RUN_ENV = 'cerebras'       # For Cerebras cloud

## Running locally using Ollama 
# LLM_MODEL = 'ollama/gemma3:1b'

## Running on Nebius 
# NEBIUS_API_KEY = your_nebius_api_key
# LLM_MODEL = 'nebius/Qwen/Qwen3-30B-A3B'

## Running on Replicate 
# REPLICATE_API_TOKEN = your_replicate_api_token
# LLM_MODEL = 'replicate/meta/meta-llama-3-8b-instruct'

## Running on Cerebras
# CEREBRAS_API_KEY = your_cerebras_api_key
# LLM_MODEL = 'cerebras/llama3.1-8b'

## ----------- Graph Builder (FREE LLM Providers) -----------
## Choose your FREE LLM provider for entity and relationship extraction
## Options: gemini, cerebras

## DEFAULT: Google Gemini (1500 requests/day FREE)
# GRAPH_LLM_PROVIDER = 'gemini'
# GEMINI_API_KEY = 'your_free_gemini_api_key'  # Get at: https://aistudio.google.com/

## OPTION 2: Cerebras API
# GRAPH_LLM_PROVIDER = 'cerebras'
# CEREBRAS_API_KEY = 'your_cerebras_api_key'

## ----------- Graph Extraction Configuration -----------
## Configure entity and relationship extraction parameters
# GRAPH_MIN_ENTITIES = 5          # Minimum entities to extract per chunk
# GRAPH_MAX_ENTITIES = 15         # Maximum entities to extract per chunk
# GRAPH_MIN_RELATIONSHIPS = 3     # Minimum relationships to extract per chunk
# GRAPH_MAX_RELATIONSHIPS = 8     # Maximum relationships to extract per chunk
# GRAPH_MIN_CONFIDENCE = 0.8      # Minimum confidence threshold for extraction
# GRAPH_MAX_CONTENT_CHARS = 12000  # Maximum characters to process per chunk
# GRAPH_SENTENCE_BOUNDARY_RATIO = 0.7  # Ratio for truncating at sentence boundary

## ----------- Neo4j Graph Database Configuration -----------
# NEO4J_URI = 'bolt://localhost:7687'
# NEO4J_USERNAME = 'neo4j'
# NEO4J_PASSWORD = 'your_password'
# NEO4J_DATABASE = 'neo4j'

## ----------- Flask Server Configuration -----------
# PORT = 8080  # Default port for the Flask application

## ----------- UI Configuration -----------
# UI_STARTER_PROMPTS = 'What is this website?  |  What are upcoming events?  | Who are some of the partners?'

